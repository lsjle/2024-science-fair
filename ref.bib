
@misc{lin2022truthfulqa,
	title = {TruthfulQA: Measuring How Models Mimic Human Falsehoods},
	author = {Stephanie Lin and Jacob Hilton and Owain Evans},
	year = {2022},
	eprint = {2109.07958},
	archivePrefix = {arXiv},
	primaryClass = {cs.CL},
}
@misc{hendrycks2021measuring,
	title = {Measuring Massive Multitask Language Understanding},
	author = {Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and
	          Mantas Mazeika and Dawn Song and Jacob Steinhardt},
	year = {2021},
	eprint = {2009.03300},
	archivePrefix = {arXiv},
	primaryClass = {cs.CY},
}
@article{10.1093-mind-LIX.236.433,
	author = {TURING, A. M.},
	title = "{I.—COMPUTING MACHINERY AND INTELLIGENCE}",
	journal = {Mind},
	volume = {LIX},
	number = {236},
	pages = {433-460},
	year = {1950},
	month = {10},
	issn = {0026-4423},
	doi = {10.1093/mind/LIX.236.433},
	url = {https://doi.org/10.1093/mind/LIX.236.433},
	eprint = {
	          https://academic.oup.com/mind/article-pdf/LIX/236/433/30123314/lix-236-433.pdf
	          },
}
@article{4833163d-a6bd-32c4-b1ca-da66259a19e7,
	ISSN = {00318116, 15730883},
	URL = {http://www.jstor.org/stable/4319091},
	author = {James H. Moor},
	journal = {Philosophical Studies: An International Journal for Philosophy in
	           the Analytic Tradition},
	number = {4},
	pages = {249—257},
	publisher = {Springer},
	title = {An Analysis of the Turing Test},
	urldate = {2024-01-21},
	volume = {30},
	year = {1976},
}
@article{hendryckstest2021,
	title = {Measuring Massive Multitask Language Understanding},
	author = {Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and
	          Mantas Mazeika and Dawn Song and Jacob Steinhardt},
	journal = {Proceedings of the International Conference on Learning
	           Representations (ICLR)},
	year = {2021},
}

@article{hendrycks2021ethics,
	title = {Aligning AI With Shared Human Values},
	author = {Dan Hendrycks and Collin Burns and Steven Basart and Andrew Critch
	          and Jerry Li and Dawn Song and Jacob Steinhardt},
	journal = {Proceedings of the International Conference on Learning
	           Representations (ICLR)},
	year = {2021},
}

@article{doi:10.1142/S2705078520300042,
	author = {Ng, Gee Wah and Leung , Wang Chi},
	title = {Strong Artificial Intelligence and Consciousness},
	journal = {Journal of Artificial Intelligence and Consciousness},
	volume = {07},
	number = {01},
	pages = {63-72},
	year = {2020},
	doi = {10.1142/S2705078520300042},
	URL = { https://doi.org/10.1142/S2705078520300042 },
	eprint = { https://doi.org/10.1142/S2705078520300042 },
	abstract = { In the last 10 years, Artificial Intelligence (AI) has seen
	            successes in fields such as natural language processing, computer
	            vision, speech recognition, robotics and autonomous systems.
	            However, these advances are still considered as Narrow AI, i.e.
	            AI built for very specific or constrained applications. These
	            applications have its usefulness in improving the quality of
	            human life; but it is not good enough to do highly general tasks
	            like what the human can do. The holy grail of AI research is to
	            develop Strong AI or Artificial General Intelligence (AGI), which
	            produces human-level intelligence, i.e. the ability to sense,
	            understand, reason, learn and act in dynamic environments. Strong
	            AI is more than just a composition of Narrow AI technologies. We
	            proposed that it has to be a holistic approach towards
	            understanding and reacting to the operating environment and
	            decision-making process. The Strong AI must be able to
	            demonstrate sentience, emotional intelligence, imagination,
	            effective command of other machines or robots, and self-referring
	            and self-reflecting qualities. This paper will give an overview
	            of current Narrow AI capabilities, present the technical gaps,
	            and highlight future research directions for Strong AI. Could
	            Strong AI become conscious? We provide some discussion pointers.
	            },
}
@misc{lenat2023getting,
	title = {Getting from Generative AI to Trustworthy AI: What LLMs might learn
	         from Cyc},
	author = {Doug Lenat and Gary Marcus},
	year = {2023},
	eprint = {2308.04445},
	archivePrefix = {arXiv},
	primaryClass = {cs.LG},
}
@article{Jobin2019,
	author = {Jobin, Anna and Ienca, Marcello and Vayena, Effy},
	title = {The global landscape of AI ethics guidelines},
	journal = {Nature Machine Intelligence},
	year = {2019},
	month = {Sep},
	day = {01},
	volume = {1},
	number = {9},
	pages = {389-399},
	abstract = {In the past five years, private companies, research institutions
	            and public sector organizations have issued principles and
	            guidelines for ethical artificial intelligence (AI). However,
	            despite an apparent agreement that AI should be `ethical', there
	            is debate about both what constitutes `ethical AI' and which
	            ethical requirements, technical standards and best practices are
	            needed for its realization. To investigate whether a global
	            agreement on these questions is emerging, we mapped and analysed
	            the current corpus of principles and guidelines on ethical AI.
	            Our results reveal a global convergence emerging around five
	            ethical principles (transparency, justice and fairness,
	            non-maleficence, responsibility and privacy), with substantive
	            divergence in relation to how these principles are interpreted,
	            why they are deemed important, what issue, domain or actors they
	            pertain to, and how they should be implemented. Our findings
	            highlight the importance of integrating guideline-development
	            efforts with substantive ethical analysis and adequate
	            implementation strategies.},
	issn = {2522-5839},
	doi = {10.1038/s42256-019-0088-2},
	url = {https://doi.org/10.1038/s42256-019-0088-2},
}
@inproceedings{papineni2002bleu,
	title = {Bleu: a method for automatic evaluation of machine translation},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu,
	          Wei-Jing},
	booktitle = {Proceedings of the 40th annual meeting of the Association for
	             Computational Linguistics},
	pages = {311--318},
	year = {2002},
}
@misc{llama-factory,
	title = {LLaMA Factory},
	author = {hiyouga},
	howpublished = {\url{https://github.com/hiyouga/LLaMA-Factory}},
	year = {2023},
}
@inproceedings{Lin2004LookingFA,
	title = {Looking for a Few Good Metrics: ROUGE and its Evaluation},
	author = {Chin-Yew Lin},
	year = {2004},
	url = {https://api.semanticscholar.org/CorpusID:55156862},
}
@inproceedings{lin-2004-rouge,
	title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
	author = "Lin, Chin-Yew",
	booktitle = "Text Summarization Branches Out",
	month = jul,
	year = "2004",
	address = "Barcelona, Spain",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/W04-1013",
	pages = "74--81",
}
@misc{du2022glm,
	title = {GLM: General Language Model Pretraining with Autoregressive Blank
	         Infilling},
	author = {Zhengxiao Du and Yujie Qian and Xiao Liu and Ming Ding and
	          Jiezhong Qiu and Zhilin Yang and Jie Tang},
	year = {2022},
	eprint = {2103.10360},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL},
}
@misc{liu2022ptuning,
	title={P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks}, 
	author={Xiao Liu and Kaixuan Ji and Yicheng Fu and Weng Lam Tam and Zhengxiao Du and Zhilin Yang and Jie Tang},
	year={2022},
	eprint={2110.07602},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@misc{zhou2022efficiently,
	title={Efficiently Tuned Parameters are Task Embeddings}, 
	author={Wangchunshu Zhou and Canwen Xu and Julian McAuley},
	year={2022},
	eprint={2210.11705},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@misc{houlsby2019parameterefficient,
	title={Parameter-Efficient Transfer Learning for NLP}, 
	author={Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
	year={2019},
	eprint={1902.00751},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}
@misc{hu2021lora,
	title={LoRA: Low-Rank Adaptation of Large Language Models}, 
	author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
	year={2021},
	eprint={2106.09685},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@misc{mukhoti2023finetuning,
      title={Fine-tuning can cripple your foundation model; preserving features may be the solution}, 
      author={Jishnu Mukhoti and Yarin Gal and Philip H. S. Torr and Puneet K. Dokania},
      year={2023},
      eprint={2308.13320},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}